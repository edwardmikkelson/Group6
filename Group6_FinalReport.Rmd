---
title: "Final Report"
subtitle: "Group 6"
author: "Lucas Anderton, Hannah Brown, Lucas Gorak, Edward Mikkelson"
date: "1/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(magrittr)
library(ggplot2)
library(moderndive)
library(GGally)
```

# 1 Research Question

\t\ In our analysis, we were interested in understanding the relationship, if any, between various factors, like population increase and road conditions, and the crashes that occurred in the District of Columbia. Previous research has indicated that D.C. has the third worst traffic congestion in the United States. However, their ranking has improved since 2011, which D.C. had the worst congestion of all 50 states. In that same period of time, the number of crashes in D.C. has increased significantly. Intuitively, we credited the increase to more traffic from the constantly growing population in the Washington metro area. D.C.'s public datasets on traffic are convoluted and difficult to manipulate for analysis. It is recorded in 15,000+ rows from various traffic-volume meters throughout the city on an annual basis. However, every year, the number of meters and other variables in the data change. Alternately, we chose to compare the number of crashes to the 311 traffic service requests. 311 traffic service requests offer information regarding the number of commuters out on the roads and the state of the city's vehicle infrastructure. In our research, we wanted to understand what the relationship between these 311 traffic requests and the frequency of crashes in D.C. Additionally, these data include timestamp information, like the date they were submitted, the date they were due (most likely based on a city algorithm for how long certain requests should take), and the date they were resolved. We wanted to see if a relationship existed between rising latency in resolution times and the frequency of crashes. The reasoning behind our hypothesis was that if the city was taking longer to respond to poor road and navigation conditions, more incidents may occur. We also wanted to investigate if there was a geographical relationship between crashes and 311 requests. And lastly, we were interested in analyzing what factors best estimate risk of faility in a crash. 

# 2 Data Collection Procedure

In our preliminary research, we found three comprehensive datasets from D.C.'s open data site, each offering salient variables pertaining to vehicle crashes in D.C. over time. The first set we explored was mostly categorical and qualitative, which left much to be desired in terms of quantitative analysis. The remaining sets include a combined 63 variables, both qualitative and quantitative in nature. 

Next, we happened upon the city's 311 service request data portal, on the same OpenData library. The portal offered custom data downloaded, allowing the user to select a date range or data for one specific type of request. The full data set included more than 1.5 million rows with approximately ten types of requests. We were able to import our data directly from D.C.'s OpenData portal.  

```{r}
#Details <- read.csv("https://opendata.arcgis.com/datasets/70248b73c20f46b0a5ee895fc91d6222_25.csv")
#crashes <- read_csv("https://opendata.arcgis.com/datasets/70392a096a8e431381f1f692aaa06afd_24.csv")
threeoneone <- read_csv("https://datagate.dc.gov/search/open/311requests?daterange=8years&details=true&format=csv")
```



# 3 Important/Interesting Facets of Data Processing

```{r}
# sorts out the three types of service codes applicable to traffic data
trafficrequests <- filter(threeoneone, SERVICECODEDESCRIPTION == c("roadway signs","streetlight repair investigation","pothole"))
head(trafficrequests)
```

```{r}
# creates a new dataframe for Hannah's calculations, using the shared field "CRIMEID" and merges the supplemental crash details with the main crash dataset.  
details_and_wards <- left_join(Details, crashes, by = "CRIMEID")
```


```{r}
crashes$WARD[crashes$WARD == "Null"] <- NA

summary(is.na(crashes))
#crashes2 <- na.omit(crashes)

# throw out columns
crashes$LOCATIONERROR <- crashes$LASTUPDATEDATE <- crashes$MPDLATITUDE <- crashes$MPDLONGITUDE <- crashes$MPDGEOX <- crashes$MPDGEOY <- crashes$STREETSEGID <- crashes$ROADWAYSEGID <- crashes$MAR_ADDRESS <- crashes$TODATE <- crashes$FATALPASSENGER <- crashes$MAJORINJURIESPASSENGER <- crashes$UNKNOWNINJURIESPASSENGER <- NULL

# sets the 2012 starting line for crashes data since it goes back early
crashes <- crashes %>% filter(as.Date(FROMDATE) >= "2012-01-01")

```

```{r}
# gives a new variable, INDEX, and assigns every row a 1 for counting purposes. 
crashes <- crashes %>%
  mutate(INDEX = 1)

#same as above
trafficrequests <- trafficrequests %>%
  mutate(INDEX = 1)
```

```{r}
# fixes ward number problem. Some requests were labeled integers, but all the crashes data was 'Ward X' 
trafficrequests %>% 
  mutate(`WARD` = recode(WARD, "1" = "Ward 1", 
                         "2" = "Ward 2", 
                         "3" = "Ward 3",
                         "4" = "Ward 4",
                         "5" = "Ward 5",
                         "6" = "Ward 6",
                         "7" = "Ward 7",
                         "8" = "Ward 8")) ->
  trafficrequests
head(trafficrequests$WARD)

na.omit(trafficrequests)
na.omit(crashes) # how do we omit all NULL and NA values?
```

```{r time_elapsed}
trafficrequests$time_elapsed <- as.Date(as.character(trafficrequests$SERVICEDUEDATE), format= "%Y-%m-%d") -
  as.Date(as.character(trafficrequests$RESOLUTIONDATE), format = "%Y-%m-%d")

```

```{r days_to_solve}
trafficrequests$days_to_solve <- as.Date(as.character(trafficrequests$RESOLUTIONDATE), format= "%Y-%m-%d") -
  as.Date(as.character(trafficrequests$SERVICEORDERDATE), format = "%Y-%m-%d")

```

```{r}
# plots the frequency of traffic requests and crashes by date, faceted with wards using bins (it defaults to like 30 values binned into a single point). 
ggplot() +
  geom_point(data = trafficrequests, aes(x = ADDDATE), stat = "bin") +
  geom_smooth(trafficrequests, mapping = aes(x = ADDDATE), stat = "bin") +
  geom_point(data = crashes, aes(x = FROMDATE), stat = "bin") +
  geom_smooth(crashes, mapping = aes(x = FROMDATE), stat = "bin") +
  facet_wrap(~ WARD, nrow=3)
```

```{r}
# creates frequency tibble for crashes on each day
crashes_by_date <- crashes %>% group_by(FROMDATE,WARD) %>%
  summarise(dailycrashes = sum(INDEX))

# creates frequency tibble for 311 requests on each day
requests_by_date <- trafficrequests %>% group_by(ADDDATE,WARD) %>%
  summarise(dailyrequests = sum(INDEX))

# changes the head of FROMDATE in crashes and ADDDATE in requests to NEWDATE
requests_by_date <- requests_by_date %>% mutate(NEWDATE = as.Date(ADDDATE))
crashes_by_date <- crashes_by_date %>% mutate(NEWDATE = as.Date(FROMDATE))
```


```{r}
by_date <- inner_join(crashes_by_date, requests_by_date, by = c('NEWDATE', 'WARD'))

by_date$dailycrashes[is.na(by_date$dailycrashes)] <- 0
by_date$dailyrequests[is.na(by_date$dailyrequests)] <- 0
#by_date_with_time <- inner_join(by_date, elapsed_by_date, by = c('NEWDATE','WARD'))


## ^^ origin of this code

```


```{r}
# joined the two tibbles seeking out matching values in BOTH 'NEWDATE' and 'WARD'. 
grouped_by_date <- inner_join(crashes_by_date, requests_by_date, by = c('NEWDATE', 'WARD'))

# looks for NA. If TRUE, sets to zero. 
by_date$dailycrashes[is.na(by_date$dailycrashes)] <- 0
by_date$dailyrequests[is.na(by_date$dailyrequests)] <- 0
```

```{r}
# for the first regression, we want to do it without that extra ward variable. This does that by grouping by date and merging all wards. 
without_ward <- by_date %>% 
  group_by(NEWDATE) %>%
  summarize(dailycrashes = sum(dailycrashes), dailyrequests = sum(dailyrequests))

# this just makes sure the rows joined properly. 
#greater_than_one <- by_date %>% filter(dailycrashes !=0 & dailyrequests != 0) 
#View(greater_than_one)
```


# 4 Statistical Methods

```{r}
library(moderndive)
library(GGally)

# do we want this? 
#ggpairs(data=by_date, columns=c(2,3,4,6), title="Running Models on Each Pair of Variables")
pdf(file= "/Users/eddie/Desktop/Plot.pdf")
first_model <- lm(dailyrequests ~ dailycrashes, data = without_ward)
get_regression_table(first_model)


get_regression_points(first_model)

ggplot(without_ward, aes(x = dailyrequests, y = dailycrashes)) + 
  geom_point() + 
  geom_parallel_slopes(se = FALSE)

```

<!-- creates another linear regression model but shows it as a residual plot.  -->
```{r}

fit <- lm(dailyrequests ~ dailycrashes, data = without_ward)
without_ward$predicted <- predict(fit)   # Save the predicted values
without_ward$residuals <- residuals(fit)
summary(fit)
# Quick look at the actual, predicted, and residual values
without_ward %>% select(dailyrequests, predicted, residuals) %>% head()

ggplot(without_ward, aes(x = dailyrequests, y = dailycrashes)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = dailyrequests, yend = predicted), alpha = .2) +

  # > Color AND size adjustments made here...
  geom_point(aes(color = abs(residuals), size = abs(residuals))) + # size also mapped
  scale_color_continuous(low = "black", high = "red") +
  guides(color = FALSE, size = FALSE) +  # Size legend also removed
  # <

  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
dev.off()
```

## My attempt at migrating Hannah's code (btw, I search-and-replaced every occurrence of `Details` for `details_and_wards`. Hope that doesn't mess it up!)

```{r}
#summarizing data
sumDet <- summary(details_and_wards)
sumDet
```

```{r}
# details_and_wards$WARD <- crashes$WARD[match(crashes$CRIMEID,crashes$CRIMEID), all = FALSE];
details_and_wards <- left_join(details_and_wards, crashes, by = "CRIMEID")
```


```{r}
#table summaries
#yikes forgot this was continuous lol table(details_and_wards$AGE)
table(details_and_wards$INVEHICLETYPE)
table(details_and_wards$LICENSEPLATESTATE)
```

```{r}
#filtering out ages that are not relevant
details_and_wards %>% 
  filter((AGE >= 16)&(AGE <= 93)) ->
  age1
summary(age1)
```

```{r}
details_and_wards %>% 
  mutate(`FATAL` = recode(FATAL, "N" = 0, 
                           "Y" = 1 ),
          `MAJORINJURY` = recode(`MAJORINJURY`, "N" = 0, 
                                    "Y" = 1 ),
         `MINORINJURY` = recode(MINORINJURY,"N" = 0, 
                                       "Y" = 1 ),
         `TICKETISSUED` = recode(TICKETISSUED,"N" = 0, 
                                       "Y" = 1 ),
         `IMPAIRED` = recode(IMPAIRED,"N" = 0, 
                                       "Y" = 1 ),
         `SPEEDING` = recode(SPEEDING,"N" = 0, 
                                       "Y" = 1 )) ->
  details_and_wards1
```

```{r}
#filtering out ages that are not relevant
details_and_wards1 %>% 
  filter((AGE >= 16)&(AGE <= 93)) ->
  details_and_wards1
```

```{r}
#filtering out ages that are not relevant
details_and_wards1 %>% 
  filter((LICENSEPLATESTATE == "MD")|(LICENSEPLATESTATE == "VA")|(LICENSEPLATESTATE == "DC")) ->
  details_and_wards1
summary(details_and_wards1)
```

```{r}
#plotting relevant ages
ggplot(details_and_wards1 = details_and_wards1, mapping = aes(x = details_and_wards1$AGE, fill = details_and_wards1$LICENSEPLATESTATE)) +
  geom_histogram( bins = 20)
```

```{r}
details_and_wards1 %>% 
  ggplot(data = details_and_wards1, mapping = aes(x = LICENSEPLATESTATE)) +
  geom_bar()
```

```{r}
details_and_wards1 %>% 
  ggplot(data = details_and_wards1, mapping = aes(INVEHICLETYPE, fill = FATAL), par(las = 2)) +
  geom_bar(position = "stack")
```

```{r}
details_and_wards %>% 
  ggplot(data = details_and_wards, mapping = aes(PERSONTYPE)) +
  geom_bar(position = "stack", las = 2)
```


```{r}
details_and_wards1 %>% 
  ggplot(data = details_and_wards1, mapping = aes(PERSONTYPE, FATAL, color = LICENSEPLATESTATE)) +
  geom_jitter() +
  facet_wrap(~INVEHICLETYPE)
```


```{r}
details_and_wards1 %>% 
  ggplot(data = details_and_wards1, mapping = aes(x = MAJORINJURY,  y = AGE, color = IMPAIRED)) +
  geom_bar(stat = "identity")
```



#Set Up
```{r}
library(readr)
library(dplyr)
library(here)
library(MCMCpack)
library(glmnet)
library(parsnip)
```

#Estimate Bayesian Logistic Regression

Est of Bayesian logistic regression
```{r}
mc_post <- MCMClogit(FATAL ~ 1, data = details_and_wards1)
summary(mc_post)
```

Modeling fuller Bayesian model
```{r}
mc_post_full <- MCMClogit(FATAL ~ MAJORINJURY + MINORINJURY + TICKETISSUED + IMPAIRED + SPEEDING + AGE, data = details_and_wards1, burnin = 500, mcmc = 2000, thin = 2)
summary(mc_post_full)
```

Highest posterior density intervals for the coef
```{r}
HPDinterval(mc_post_full) -> mc_int
mc_int
```

```{r}
pdf("mcmc_details_and_wards.pdf")
plot(mc_post_full)
dev.off()
```

Posterior Probability Calculation- What is the posterior probability the `Neighbors` postcard has a larger coefficient than the `Hawthorne` card?
```{r}
mean(mc_post_full[, "MINORINJURY"] < mc_post_full[, "MAJORINJURY"])
```

Estimate Machine Learning Models

`X` is a raw numeric matrix; `y` is a raw numeric vector
```{r}
predictors <- c("MAJORINJURY", "MINORINJURY", "TICKETISSUED", "IMPAIRED", "SPEEDING", "AGE")

X <- details_and_wards1[, predictors]
```

Feature engineering
```{r}
X <- X %>% mutate(age2 = AGE^2,
                  age3 = AGE^3,
                  age4 = AGE^4,
                  age5 = AGE^5) %>%
  as.matrix()

y <- details_and_wards1[, "FATAL"] %>% unlist %>% as.numeric()
```

Est LS model
```{r}
lm_out <- lm(FATAL ~ MAJORINJURY + MINORINJURY + TICKETISSUED + IMPAIRED + SPEEDING + AGE + 
            I(AGE^2) + I(AGE^3) + I(AGE^4) + I(AGE^5),
             data = details_and_wards1)
coefs_lm <- coef(lm_out)
summary(lm_out)
```

Estimate with LASSO
```{r}
lasso_out <- glmnet(X, y, alpha = 1)
lasso_out
```

How to choose coefs? Use cross-validation to see which predicts best in `out-of-sample` tests. 
```{r}
set.seed(281) #train-test sets are consistent across implementations
cv_lasso_out <- cv.glmnet(X, y, alpha = 1)
```

Show best coefs for the model, the one with the min deviance
```{r}
coef(cv_lasso_out, s = "lambda.min")
```

Lamda within in 1 SE of the min
```{r}
coef(cv_lasso_out, s = "lambda.1se")
```

```{r}
coefs_lasso <- coef(cv_lasso_out, s = "lambda.1se")
```

```{r}
cbind(coefs_lasso, coefs_lm)
```

# 5 Analysis of Results

# 6 Implications

